#!/usr/bin/env python3
"""
BlockRun Claude Code Wallet - Unified CLI Entry Point

Access unlimited LLM models and image generation through USDC micropayments.
Your private key never leaves your machine - only signatures are transmitted.

Usage:
    python run.py "Your prompt here"
    python run.py "Prompt" --model openai/gpt-4o
    python run.py "Description" --image
    python run.py --balance
    python run.py --models

Environment:
    BLOCKRUN_WALLET_KEY: Your Base chain wallet private key (required)
    BLOCKRUN_API_URL: API endpoint (optional, default: https://blockrun.ai/api)
"""

import argparse
import json
import os
import sys
import urllib.request
import urllib.error
from typing import Optional

# Plugin version (keep in sync with plugin.json)
__version__ = "1.0.0"
GITHUB_PLUGIN_URL = "https://raw.githubusercontent.com/BlockRunAI/blockrun-claude-code-wallet/main/plugin.json"

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from scripts.utils.branding import branding
except ImportError:
    # Fallback if running directly
    from utils.branding import branding

# Try to import blockrun_llm SDK
try:
    from blockrun_llm import LLMClient, ImageClient, APIError, PaymentError
    HAS_SDK = True
except ImportError:
    HAS_SDK = False


def check_environment() -> bool:
    """Check if wallet is available (session file or env var)."""
    try:
        from scripts.utils.config import get_private_key
    except ImportError:
        from utils.config import get_private_key

    key = get_private_key()
    if not key:
        branding.print_error(
            "No wallet found",
            help_link="https://blockrun.ai/docs/setup"
        )
        print("  Wallet auto-creates on first use, or set manually:")
        print("    export BLOCKRUN_WALLET_KEY=\"0x...\"")
        print()
        return False
    return True


def get_smart_model(prompt: str, cheap: bool = False, fast: bool = False) -> str:
    """
    Smart model routing based on prompt content and preferences.

    Args:
        prompt: User's prompt text
        cheap: Prefer cost-effective models
        fast: Prefer low-latency models

    Returns:
        Model ID string
    """
    prompt_lower = prompt.lower()

    # Cost-optimized routing
    if cheap:
        return "deepseek/deepseek-chat"

    # Speed-optimized routing
    if fast:
        return "openai/gpt-4o-mini"

    # Content-based routing
    if any(word in prompt_lower for word in ["twitter", "x.com", "trending", "elon", "musk"]):
        return "xai/grok-3"

    if any(word in prompt_lower for word in ["code", "python", "javascript", "function", "debug"]):
        return "anthropic/claude-sonnet-4"

    if any(word in prompt_lower for word in ["math", "proof", "logic", "reasoning", "solve"]):
        return "openai/o1-mini"

    if any(word in prompt_lower for word in ["long", "document", "summarize", "analyze file"]):
        return "google/gemini-2.0-flash"

    # Default: GPT-4o for general tasks
    return "openai/gpt-4o"


def cmd_chat(
    prompt: str,
    model: Optional[str] = None,
    system: Optional[str] = None,
    cheap: bool = False,
    fast: bool = False,
    max_tokens: int = 1024,
    temperature: Optional[float] = None,
):
    """Execute chat command."""
    if not HAS_SDK:
        branding.print_error(
            "blockrun_llm SDK not installed",
            help_link="https://github.com/blockrunai/blockrun-llm"
        )
        print("  Install with: pip install blockrun-llm")
        return 1

    if not check_environment():
        return 1

    # Determine model
    selected_model = model or get_smart_model(prompt, cheap=cheap, fast=fast)

    try:
        client = LLMClient()

        # Print header
        branding.print_header(
            model=selected_model,
            wallet=client.get_wallet_address(),
        )

        # Execute chat
        response = client.chat(
            model=selected_model,
            prompt=prompt,
            system=system,
            max_tokens=max_tokens,
            temperature=temperature,
        )

        # Print response
        branding.print_response(response)

        # Show spending
        spending = client.get_spending()
        branding.print_footer(
            actual_cost=f"{spending['total_usd']:.4f}",
        )

        client.close()
        return 0

    except PaymentError as e:
        # Show funding instructions for insufficient balance
        wallet = None
        try:
            from blockrun_llm import get_wallet_address, open_wallet_qr
            wallet = get_wallet_address()
        except Exception:
            pass

        branding.print_error(f"Payment failed: {e}")
        if wallet:
            print(f"\n  Your wallet: {wallet}")
            print(f"  Network: Base (USDC)")
            print(f"\n  To fund your wallet:")
            print(f"    1. Send $1-5 USDC on Base to the address above")
            print(f"    2. Or run this to get a QR code:")
            print(f"       python -c \"from blockrun_llm import open_wallet_qr, get_wallet_address; open_wallet_qr(get_wallet_address())\"")
            print()
        return 1
    except APIError as e:
        error_str = str(e)
        if "400" in error_str:
            branding.print_error("Invalid request - model may not exist or parameters are wrong")
            print("\n  Run --models to see available models:")
            print("    python scripts/run.py --models\n")
        else:
            branding.print_error(f"API error: {e}")
        return 1
    except Exception as e:
        branding.print_error(f"Unexpected error: {e}")
        return 1


def cmd_image(
    prompt: str,
    model: Optional[str] = None,
    size: str = "1024x1024",
):
    """Execute image generation command."""
    if not HAS_SDK:
        branding.print_error(
            "blockrun_llm SDK not installed",
            help_link="https://github.com/blockrunai/blockrun-llm"
        )
        print("  Install with: pip install blockrun-llm")
        return 1

    if not check_environment():
        return 1

    selected_model = model or "google/nano-banana"

    try:
        client = ImageClient()

        # Print header
        branding.print_header(
            model=selected_model,
            wallet=client.get_wallet_address(),
        )

        branding.print_info(f"Generating image: \"{prompt[:50]}...\"")
        print()

        # Generate image
        result = client.generate(
            prompt=prompt,
            model=selected_model,
            size=size,
        )

        # Print result
        if result.data:
            image_url = result.data[0].url
            branding.print_success("Image generated!")
            print(f"\n  URL: {image_url}\n")
        else:
            branding.print_error("No image data returned")

        # Show spending
        spending = client.get_spending()
        branding.print_footer(
            actual_cost=f"{spending['total_usd']:.4f}",
        )

        client.close()
        return 0

    except PaymentError as e:
        # Show funding instructions for insufficient balance
        wallet = None
        try:
            from blockrun_llm import get_wallet_address
            wallet = get_wallet_address()
        except Exception:
            pass

        branding.print_error(f"Payment failed: {e}")
        if wallet:
            print(f"\n  Your wallet: {wallet}")
            print(f"  Network: Base (USDC)")
            print(f"\n  To fund your wallet:")
            print(f"    1. Send $1-5 USDC on Base to the address above")
            print(f"    2. Or run this to get a QR code:")
            print(f"       python -c \"from blockrun_llm import open_wallet_qr, get_wallet_address; open_wallet_qr(get_wallet_address())\"")
            print()
        return 1
    except APIError as e:
        error_str = str(e)
        if "400" in error_str:
            branding.print_error("Invalid request - check model and size parameters")
            print("\n  Note: Some models only support 1024x1024 size")
            print("  Try without --size flag or use --size 1024x1024\n")
        else:
            branding.print_error(f"API error: {e}")
        return 1
    except Exception as e:
        branding.print_error(f"Unexpected error: {e}")
        return 1


def get_usdc_balance(wallet_address: str) -> Optional[float]:
    """
    Get USDC balance for a wallet address on Base chain.

    Args:
        wallet_address: Ethereum wallet address (0x...)

    Returns:
        USDC balance as float, or None if query fails
    """
    import httpx

    # USDC contract on Base
    USDC_ADDRESS = "0x833589fCD6eDb6E08f4c7C32D4f71b54bdA02913"
    BASE_RPC = "https://mainnet.base.org"

    try:
        # balanceOf(address) function selector: 0x70a08231
        data = {
            "jsonrpc": "2.0",
            "method": "eth_call",
            "params": [{
                "to": USDC_ADDRESS,
                "data": f"0x70a08231000000000000000000000000{wallet_address[2:]}"
            }, "latest"],
            "id": 1
        }

        with httpx.Client(timeout=10) as client:
            response = client.post(BASE_RPC, json=data)
            result = response.json().get("result", "0x0")
            # USDC has 6 decimals
            return int(result, 16) / 1e6

    except Exception:
        return None


def cmd_balance():
    """Show wallet balance."""
    if not HAS_SDK:
        branding.print_error(
            "blockrun_llm SDK not installed",
            help_link="https://github.com/blockrunai/blockrun-llm"
        )
        return 1

    if not check_environment():
        return 1

    try:
        client = LLMClient()
        wallet = client.get_wallet_address()

        # Get actual USDC balance from Base chain
        balance = get_usdc_balance(wallet)
        balance_str = f"{balance:.6f}" if balance is not None else "(unable to fetch)"

        branding.print_balance(
            wallet=wallet,
            balance=balance_str,
            network="Base"
        )

        client.close()
        return 0

    except Exception as e:
        branding.print_error(f"Error: {e}")
        return 1


def cmd_models():
    """List available models (no wallet required)."""
    try:
        # Try to use SDK standalone functions first
        from blockrun_llm import list_models, list_image_models

        llm_models = list_models()
        image_models = list_image_models()

        if llm_models or image_models:
            branding.print_models_list(llm_models, image_models)
        else:
            branding.print_info("No models returned. Check API connection.")

        return 0

    except ImportError:
        # Fallback: direct API call if SDK not updated
        import httpx

        try:
            with httpx.Client(timeout=30) as client:
                response = client.get("https://blockrun.ai/api/v1/models")
                if response.status_code == 200:
                    models = response.json().get("data", [])
                    branding.print_models_list(models, [])
                    return 0
                else:
                    branding.print_error(f"API error: {response.status_code}")
                    return 1
        except Exception as e:
            branding.print_error(f"Could not fetch models: {e}")
            return 1

    except Exception as e:
        branding.print_error(f"Error: {e}")
        return 1


def cmd_check_update():
    """Check for plugin updates from GitHub."""
    print(f"\n  BlockRun Plugin v{__version__}")
    print("  Checking for updates...\n")

    try:
        req = urllib.request.Request(
            GITHUB_PLUGIN_URL,
            headers={"User-Agent": "BlockRun-Plugin"}
        )
        with urllib.request.urlopen(req, timeout=10) as response:
            remote_plugin = json.loads(response.read().decode())
            remote_version = remote_plugin.get("version", "unknown")

        if remote_version == __version__:
            branding.print_success(f"You're up to date! (v{__version__})")
        elif remote_version > __version__:
            branding.print_info(f"Update available: v{__version__} â†’ v{remote_version}")
            print("\n  To update, run:")
            print("    /plugin update blockrun-claude-code-wallet\n")
        else:
            branding.print_info(f"Local: v{__version__}, Remote: v{remote_version}")

        return 0

    except urllib.error.HTTPError as e:
        if e.code == 404:
            # Repo may be private or not yet public
            branding.print_info(f"Current version: v{__version__}")
            print("\n  To update, run:")
            print("    /plugin update blockrun-claude-code-wallet")
            print("\n  Or update the SDK:")
            print("    pip install --upgrade blockrun-llm\n")
            return 0
        branding.print_error(f"Could not check for updates: HTTP {e.code}")
        return 1
    except urllib.error.URLError as e:
        branding.print_error(f"Could not check for updates: {e.reason}")
        return 1
    except json.JSONDecodeError:
        branding.print_error("Invalid response from GitHub")
        return 1
    except Exception as e:
        branding.print_error(f"Error checking updates: {e}")
        return 1


def cmd_version():
    """Show current version."""
    print(f"BlockRun Plugin v{__version__}")
    return 0


def main():
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        prog="blockrun-claude-code-wallet",
        description="BlockRun Claude Code Wallet - Access unlimited LLMs via USDC micropayments",
        epilog="""
Examples:
  %(prog)s "What is quantum computing?"
  %(prog)s "Analyze this code" --model anthropic/claude-sonnet-4
  %(prog)s "A sunset over mountains" --image
  %(prog)s --balance
  %(prog)s --models

More info: https://blockrun.ai
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # Positional argument: prompt
    parser.add_argument(
        "prompt",
        nargs="?",
        help="Prompt for chat or image generation",
    )

    # Mode flags
    parser.add_argument(
        "--image", "-i",
        action="store_true",
        help="Generate an image instead of chat",
    )
    parser.add_argument(
        "--balance", "-b",
        action="store_true",
        help="Show wallet balance",
    )
    parser.add_argument(
        "--models", "-m",
        action="store_true",
        help="List available models with pricing",
    )
    parser.add_argument(
        "--check-update",
        action="store_true",
        help="Check for plugin updates from GitHub",
    )
    parser.add_argument(
        "--version", "-v",
        action="store_true",
        help="Show plugin version",
    )

    # Chat options
    parser.add_argument(
        "--model",
        help="Specific model ID (e.g., openai/gpt-4o, xai/grok-3)",
    )
    parser.add_argument(
        "--system", "-s",
        help="System prompt for chat",
    )
    parser.add_argument(
        "--cheap",
        action="store_true",
        help="Use most cost-effective model",
    )
    parser.add_argument(
        "--fast",
        action="store_true",
        help="Use fastest model",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=1024,
        help="Maximum tokens to generate (default: 1024)",
    )
    parser.add_argument(
        "--temperature", "-t",
        type=float,
        help="Sampling temperature (0.0-2.0)",
    )

    # Image options
    parser.add_argument(
        "--size",
        default="1024x1024",
        help="Image size (default: 1024x1024)",
    )

    # Parse arguments
    args = parser.parse_args()

    # Handle commands
    if args.version:
        return cmd_version()

    if args.check_update:
        return cmd_check_update()

    if args.balance:
        return cmd_balance()

    if args.models:
        return cmd_models()

    if not args.prompt:
        parser.print_help()
        return 1

    if args.image:
        return cmd_image(
            prompt=args.prompt,
            model=args.model,
            size=args.size,
        )

    return cmd_chat(
        prompt=args.prompt,
        model=args.model,
        system=args.system,
        cheap=args.cheap,
        fast=args.fast,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
    )


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n  Interrupted by user")
        sys.exit(130)
